<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <script src="https://distill.pub/template.v2.js"></script>
        <script src="https://d3js.org/d3.v5.min.js"></script>
        <script src="https://d3js.org/d3-collection.v1.min.js"></script>
        <script src="https://rawgit.com/nstrayer/slid3r/master/dist/slid3r.js"></script>
        <script src="cross_fade.js"></script>
        <link rel="stylesheet" href="style.css">
        
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
    </head>
    <body>
        <d-front-matter>
        <script type="text/json">{
          "title": "Visualizing the Impact of Feature Attribution Baselines",
          "description": "Exploring the baseline input hyper-parameter, and how it impacts interpretations of neural network behavior.",
          "authors": [
            {
              "author": "Pascal Sturmfels",
              "authorURL": "https://psturmfels.github.io/",
              "affiliation": "University of Washington",
              "affiliationURL": "https://www.cs.washington.edu"
            },
            {
              "author": "Scott Lundberg",
              "authorURL": "https://scottlundberg.com",
              "affiliation": "University of Washington",
              "affiliationURL": "https://www.cs.washington.edu"
          },
          {
            "author": "Su-In Lee",
            "authorURL": "https://suinlee.cs.washington.edu/su-in-lee",
            "affiliation": "University of Washington",
            "affiliationURL": "https://www.cs.washington.edu"
          }
          ]
        }</script>
      </d-front-matter>
    <d-article>
        <p>Feature attribution methods are a popular way of explaining
           deep models. Most require choosing a
           hyper-parameter known as the "baseline input".
           What does this hyper-parameter mean, and how important is it? In this article,
           we investigate these questions, and discuss why it is useful 
           to extend the notion of a baseline from a single input to a distribution. We focus
           on the Integrated Gradients method as a case study. However, our
           discussion of baselines is applicable to many existing feature attribution methods. </p>

        <h2>Introduction</h2>
        <p>
            If you are in the business of training neural networks,
            you might have heard of the integrated gradients method, which
            was introduced at 
            <a href="https://en.wikipedia.org/wiki/International_Conference_on_Machine_Learning" target="-_blank">ICML</a> two years ago 
            <d-cite key="sundararajan2017axiomatic"></d-cite>.
            The method computes which features are important 
            to a neural network in making a prediction on a 
            particular data point, which helps users
            understand what patterns their models are learning.
            Since its introduction,
            integrated gradients has been used to interpret 
            networks trained on a variety of data types, 
            including retinal fundus images <d-cite key="sayres2019using"></d-cite>
            and electrocardiogram recordings <d-cite key="warrick2018ensembling"></d-cite>.
        </p>
        <p>
            If you've ever used integrated gradients, 
            or for that matter, any of a similar family of
            attribution methods <d-cite key="lundberg2017unified,binder2016layer,shrikumar2017learning"></d-cite>,
            you also know that you need to define a baseline input \(x'\) before
            using the method. There is scarce information about this hyper-parameter choice
            in the aforementioned literature. Is integrated gradients sensitive to the 
            hyper-parameter choice? Are there any guidelines for choosing \(x'\) other
            than setting it to be a vector of zeros?
            And why does the method require this hyper-parameter in the first place?
        </p>
        <p>
            In this article, we will delve into how this hyper-parameter choice arises,
            and why understanding it is important when you are doing model interpretation.
            As a case-study, we will focus on image classification models in order 
            to visualize the effects of the baseline input. Our visualizations will
            lead us to describe an extension of integrated gradients, called <i>expected gradients</i>, which we 
            introduced in <d-cite key="erion2019learning"></d-cite>. In brief,
            expected gradients extends integrated gradients by using a data distribution
            as a baseline instead of a single input.
        </p>
        <h2>Image Classification</h2>
        <p>
          We focus on image classification as a task, as it will allow us to visually
          plot integrated gradients attributions, and compare them with our intuition
          about which pixels we think should be important. We use on the Inception V4 architecture 
          <d-cite key="szegedy2017inception"></d-cite>, a convolutional 
          neural network designed for the ImageNet dataset <d-cite key="deng2009imagenet"></d-cite>,
          in which the task is to determine which class an image belongs to out of 1000 classes.
          On the ImageNet validation set, Inception V4 has a top-1 accuracy of over 80%.
          We download weights from TensorFlow-Slim <d-cite key="silberman2016tensorflow"></d-cite>,
          and visualize the predictions of the network on four different images from the 
          validation set.
          <d-figure>
              <figure class='l-body'>
                  <div id='figure1_div'></div>
                  <script src="figure1.js"></script>
                  <figcaption>
                      The Inception V4 network classifies all images above correctly with high confidence.
                  </figcaption>
              </figure>
          </d-figure>
          
        </p>
        <p>
          Although state of the art models perform well on unseen data,
          users may still be left wondering: <i>how</i> did the model make
          the prediction it did? This question may be critical when models
          are making high stakes decisions about, say, an individual's health or finances.
          Although there are many ways to interpret deep neural networks 
          <d-cite key="olah2018the,olah2017feature,kim2017interpretability,fong2017interpretable,selvaraju2017grad,zeiler2014visualizing"></d-cite>, here we focus 
          on feature attributions.
        </p>
        <p>
          Given a target input \(x\) and a network function \(f\), 
          feature attribution methods assign a weight \(\phi_i(f, x)\)
          to the \(i\)th feature value representing how important that
          feature was toward computing \(f(x)\). By assigning numerical importance
          to each feature, we can both interpret what trends our models
          are learning and verify that those trends conform to human intuition. 
          There are many methods that compute attributions for neural networks, including DeepLIFT <d-cite key="shrikumar2017learning"></d-cite>,
          layer-wise relevance propagation <d-cite key="binder2016layer"></d-cite>
          and integrated gradients <d-cite key="sundararajan2017axiomatic"></d-cite>.
        </p>
        <p>
            Below, we visualize which pixels were most important to the network's correct
            prediction using integrated gradients.<d-footnote>Throughout this paper, we 
            use 500 samples when approximating any integral or expectation. However,
            we only display images from every 10th sample in order to avoid
            loading 500+ images for each figure.</d-footnote>
            The pixels in white indicate more important pixels. In order to plot
            attributions, we follow the same design choices as <d-cite key="smilkov2017smoothgrad"></d-cite>.
            That is, we plot the absolute value of the sum of feature attributions
            across the channel dimension, and cap feature attributions at the 99th percentile to avoid
            high-magnitude attributions dominating the color scheme.
            <d-figure>
                <figure class="l-body">
                    <div id='figure2_div'></div>
                    <script src="figure2.js"></script>
                    <figcaption>
                        Pixel-wise attributions of the Inception V4 network using Integrated Gradients.
                        You might notice that some attributions highlight pixels that do not seem important
                        relative to the true class label.
                    </figcaption>
                </figure>
            </d-figure>
        </p>
        <h2>A Better Understanding of Integrated Gradients</h2>  
        <p>
          As you look through the attribution maps, you might find some of them
          unintuitive. Why does the attribution for "goldfinch" highlight the green background?
          Why doesn't the attribution for "killer whale" highlight the black parts of the killer whale?
          To better understand this behavior, we need to explore how
          we generated feature attributions. Formally, integrated gradients
          defines the importance value for the \(i\)th feature value as follows:
          $$\phi_i^{IG}(f, x, x') = (x_i - x'_i) \times \int_{\alpha = 0}^ 1
            \frac{\delta f(x' + \alpha (x - x'))}{\delta x_i} d \alpha
          $$
          where \(x\) is the current input,
          \(f\) is the model function and \(x'\) is some baseline input that is meant to represent 
          "absence" of feature input. The subscript \(i\) is used
          to denote indexing into the \(i\)th feature.
        </p>
        <p>
            What is this formula doing? Recall that the derivative of a function
            represents rate of change. The derivative of our network with respect
            to an output class represents which pixels would most increase or
            decrease the likelihood of that class if perturbed. In a linear model,
            the pixels with the highest magnitude gradients would have the most
            impact on the model's output.
        </p>
        <p>
            Unfortunately, there are many problems with using gradients to interpret
            deep neural networks. One specific issue is that neural networks are prone to a problem
            known as saturation <d-cite key="glorot2010understanding,sundararajan2016gradients"></d-cite>.
            The gradients of input features may have small magnitudes around a 
            sample even if the network depends heavily on those features. This can happen
            if the network function flattens after those features reach a certain magnitude.
            Intuitively, shifting the pixels in an image by a small amount typically
            doesn't change what the network sees in the image. We can illustrate
            saturation by plotting the network output at all
            images between the baseline \(x'\) and the current image. The network
            output for the correct class increases initially, but then quickly flattens.
            <d-figure>
                <figure class="l-body">
                    <div id='figure3_div'></div>
                    <script src="figure3.js"></script>
                    <figcaption>
                        A plot of network outputs at \(x' + \alpha (x - x')\).
                        Notice that the network output saturates the correct class
                        at small values of \(\alpha\). By the time \(\alpha = 1\),
                        the network output barely changes.
                    </figcaption>
                </figure>
            </d-figure>
        </p>
        <p>
            What we really want to know is how our network got from 
            predicting essentially nothing at \(x'\) to being 
            completely saturated towards the correct output class at \(x\).
            Which pixels, when scaled along this path, most
            increased the network output for the correct class?
            Unfortunately, gradients can only tell us local rate of change.
            We want the average rate of change at all points 
            between our baseline and our current input. 
        </p>
        <p>
            With this view, we can see what integrated gradients is doing: 
            it is simply accumulating all the gradients along the path we've plotted
            above. By integrating over a path, 
            the method avoids problems with local gradients being
            saturated. We can break the original equation
            down and visualize it in three separate parts: (1) interpolating between
            the baseline image and the target image, (2) the gradients at the interpolated
            image times the difference between the baseline and the target images, and (3) 
            accumulating these gradients over the interpolation constant, \(\alpha\).
            $$
                \tag{1} x' + \alpha (x - x')
            $$
            $$
                \tag{2} (x_i - x'_i) \times \frac{\delta f(x' + \alpha (x - x'))}{\delta x_i}
            $$
            $$
                \tag{3} (x_i - x'_i) \times \int_{\alpha' = 0}^{\alpha} \frac{\delta f(x' + \alpha' (x - x'))}{\delta x_i} d \alpha'
            $$
            We visualize these three equations below.<d-footnote>Note that in practice, we use a discrete sum
            approximation of the integral with 50 linearly-spaced points between 0 and 1. 50 points
            seems to be a fairly standard number to use
            when using the discrete sum approximation of the integral, 
            which was also described in the original integrated gradients paper.</d-footnote>
            <d-figure>
                <figure class="l-page">
                    <div id='figure4_div'></div>
                    <script src="figure4.js"></script>
                    <figcaption>
                        Integrated Gradients, visualized. In the line chart, the red line refers to
                        equation (4) and the blue line refers to \(f(x) - f(x')\). Notice how high magnitude gradients
                        accumulate at small values of \(\alpha\).
                    </figcaption>
                </figure>
            </d-figure>
        </p>
        <p>
            The line chart above plots the following equation:
            $$
                \tag{4} \sum_i \bigg( (x_i - x'_i) \times \int_{\alpha' = 0}^{\alpha} 
                \frac{\delta f(x' + \alpha' (x - x'))}{\delta x_i} d \alpha' \bigg)
            $$
            That is, it sums all of the pixel attributions in equation (3).
            This line plot lets us see where the highest magnitude gradients
            appear in the integral. We can see saturation effects
            in this plot: the highest magnitude gradients tend to be at low values of 
            \(\alpha\), which matches our earlier plot.
        </p>
        <p>
            But there is another reason to plot 
            the sum of the feature attributions. 
            Integrated gradients attributions satisfy the
            following axiom:<d-footnote>This is thanks to
            the fundamental theorem of calculus. The discrete
            approximation of the integral only
            satisfies it in the limit, but it does converge to
            a good approximation given enough samples. </d-footnote>
            $$
                \textrm{Axiom 1: Completeness}\\
                \sum_i \phi_i^{IG}(f, x, x') = f(x) - f(x')
            $$ 
            Intuitively, this axiom is desirable because it means
            that the attribution values are breaking down the output of the network
            in terms of the importance of the input features. It also
            helps us determine whether our approximation of the integral
            is converging to the true value, or whether we need to use 
            more points. More generally, this axiom is part of a larger
            underlying theory that uniquely characterizes integrated gradients. 
        </p>
        
        <h2>Game Theory and Missingness</h2>
        <p>
            Integrated Gradients is inspired by work
            from cooperative game theory, specifically the Aumann-Shapley value
            <d-cite key="aumann2015values"></d-cite>. In cooperative game theory,
            a non-atomic game is a construction used to model large-scale economic systems
            where there are enough participants that it is desirable to model them continuously.
            Aumann-Shapley values provide a theoretically grounded way to
            determine how much different groups of participants contribute to the system.
        </p>
        <p>
            In game theory, a notion of "missingness" is well-defined. Games are defined
            on coalitions - sets of participants - and for any specific coalition,
            a participant of the system can be in or out of that coalition. The fact
            that games can be evaluated on coalitions is the foundation of
            the Aumann-Shapley value. Intuitively, it computes how
            much value a group of participants adds to the game 
            by computing how much the value of the game would increase
            if we added more of that group to any given coalition.
        </p>
        <p>
            Unfortunately, "missingness" is a more difficult notion when
            we are speaking about machine learning models. In order
            to evaluate how important the \(i\)th feature is, we
            want to be able to compute how much the output of
            the network would increase if we successively increase
            the "presence" of the \(i\)th feature. But what does this mean, exactly?
            In order to increase the presence of a feature, we would need to start
            with the feature being "missing" and have a way of interpolating 
            between that "missingness" towards its current, known value.
        </p>
        <p>
            Hopefully, this is sounding awfully familiar. Integrated gradients
            has a baseline input \(x'\) for exactly this reason: to model a
            feature being absent. But how should you choose
            \(x'\) in order to best represent this? It seems to be common practice
            to choose a baseline input \(x'\) to be the vector of
            all zeros. But consider the following scenario: you've learned a model
            on a healthcare dataset, and one of the features is blood sugar level.
            The model has correctly learned that excessively low levels of blood sugar,
            which correspond to hypoglycemia, is dangerous. Does
            a blood sugar level of \(0\) seem like a good choice to represent missingness?
        </p>
        <p>
            The point here is that fixed feature values may have unintended meaning.
            The problem compounds further when you consider the mathematical formulation
            of integrated gradients: it has a multiplicative \(x_i - x'_i\) term.<d-footnote>This term arises
            as a consequence of the completeness axiom, which requires that the derivative of
            the network be respect to the interpolation constant in the path integral.</d-footnote>
            For the sake of a thought experiment, suppose a patient had a blood sugar level of \(0\). 
            To understand why our machine learning model thinks this patient
            is at high risk, you run integrated gradients on this data point with a
            baseline of the all-zeros vector. The blood sugar level of the patient would have \(0\) feature importance,
            because \(x_i - x'_i = 0\). This is despite the fact that 
            a blood sugar level of \(0\) would be fatal!
        </p>
        <p>
          We find similar problems when we move to the image domain.
          The all-zeros image is a constant black image, which means
          that integrated gradients with this baseline 
          will not highlight black pixels as important.
          The method is blind to the color you use as a baseline, which
          we illustrate with the figure below.
          <d-figure>
              <figure class="l-page">
                  <div id='figure5_div'></div>
                  <script src="figure5.js"></script>
                  <figcaption>
                      Mouse over the segmented image to choose a different color
                      as a baseline input \(x'\). Notice that pixels
                      of the baseline color are not highlighted as important, 
                      even if they make up part of the main object in the image.
                  </figcaption>
              </figure>
          </d-figure>
          
        </p>
        <h2>Integrating out the Baseline</h2>
        <p>
            It seems like no matter what we choose \(x'\) to be,
            we will run into this issue: there will be a color
            that integrated gradients does not highlight as important. 
            So is it possible to define a more meaningful notion of a missing feature?
            We can re-frame the notion by thinking of a missing feature as <i>lack of information</i>.
            A missing pixel is a pixel whose value we do not yet know, but could assume
            any value in our dataset. If our dataset was
            mostly comprised of images of goldfinches, we might assume that
            a randomly drawn pixel from the dataset would be likely to be yellow!
        </p>
        <p>
            In supervised learning, we make the assumption that the data is drawn
            from some distribution \(D\). This assumption that the training and testing data 
            share a common, underlying distribution is what allows us to 
            do supervised learning and make claims about generalizability. If
            the data distribution has density \(p_D\), we don't need to
            model missingness using a single baseline. We can use \(p_D\) to integrate
            over all possible baselines, weighted by their density:
            $$ \phi_i(f, x) = \int_{x'} \bigg( (x_i - x'_i) \times \int_{\alpha = 0}^ 1
              \frac{\delta f(x' + \alpha (x - x'))}{\delta x_i} d \alpha \bigg) p_D(x') dx'
            $$
        </p>
        <p>
            In practice, we do not have access to the underlying distribution \(p_D\).
            However, we can observe that both of these integrals integrate over distributions.
            The first integrates over \(p_D\), and the second integrates over the uniform distribution.
            By thinking about these integrals as expectations, we arrive 
            at our proposed extension of integrated gradients, called <b>expected 
            gradients</b>, which we introduced <d-footnote>
            Our work that introduces expected gradients does not fully discuss the nuances presented here. Instead, 
            it is about how you can train models to be 
            more interpretable and achieve better generalization performance by imposing priors
            on their feature attributions.
            </d-footnote> in <d-cite key="erion2019learning"></d-cite>. We collapse both expectations to get the formal definition
            of expected gradients:
            $$ \phi_i^{EG}(f, x) = \mathop{\mathbb{E}}_{x' \sim D, \alpha \sim U(0, 1)} \bigg[ (x_i - x'_i) \times 
              \frac{\delta f(x' + \alpha (x - x'))}{\delta x_i} \bigg]
            $$
        </p>
        <p>
          Under the assumption that our training dataset represents 
          random samples from \(D\), we can draw \(x'\) randomly
          from our training set, \(\alpha\) from the uniform distribution and
          use these samples to approximately compute the expectation above.
          We illustrate this sampling process by breaking it down into three equations: 
          (1) interpolating between the \(k\)th baseline sample and the target input, (2) 
          the gradients at the interpolated image times the difference between the \(k\)th baseline sample
          and the target image, and (3) accumulating these gradients over multiple different
          reference samples,
          $$
            \tag{1} x'^k + \alpha^{k} (x - x'^k)
          $$
          $$
            \tag{2} (x_i - x'^k_i) \times 
              \frac{\delta f(x'^k + \alpha^{k} (x - x'^k))}{\delta x_i}
          $$
          $$
            \tag{3} \frac{1}{k} \sum_{j=1}^k (x_i - x'^j_i) \times 
              \frac{\delta f(x'^j + \alpha^{j} (x - x'^j))}{\delta x_i}
          $$
          where \(x'^k\) and \(x'^j\) represent the \(k\)th and \(j\)th
          samples from \(D\), \(\alpha^k\) and \(\alpha^j\) 
          are sampled uniformly from the interval \([0, 1]\),
          and \(k\) is the current sample number.
          <d-figure>
              <figure class="l-page-outset">
                  <div id='figure6_div'></div>
                  <script src="figure6.js"></script>
                  <figcaption>
                      A visual representation of expected gradients. Instead of taking contributions
                      from a single path, expected gradients averages contributions from 
                      all paths defined by the underlying data distribution. Note that
                      this figure only displays every 10th sample to avoid loading many images.
                  </figcaption>
              </figure>
          </d-figure>
        </p>
        
        <p>
          Integrated gradients is part of a family of methods called path methods,
          which integrate contributions along a path between two points. Diagonal path methods 
          are the only methods that satisfy certain axioms, like the aforementioned completeness axiom
           <d-cite key="friedman2004paths"></d-cite>. It turns out that
           expected gradients also satisfies these axioms. The key difference is that expected gradients
          allows contributions over multiple paths, not the single path between a
          fixed \(x'\) and \(x\). Other than that, it is still also a diagonal path
          method, and in expectation, satisfies the same theoretical guarantees that 
          integrated gradients does.
        </p>
        <p>
            If you are interested in using expected gradients to explain
            your machine learning models, head over 
            to the <a href="http://github.com/slundberg/shap" target="_blank">SHAP package</a> 
            <d-cite key="lundberg2017unified"></d-cite>,
            where it is implemented as the <d-code language="python">GradientExplainer</d-code>
            object for TensorFlow, Keras, and PyTorch models.
        </dt>
        </p>
        
        <h2>Conclusion</h2>
        <p>
            The purpose of this article is not to criticize integrated gradients.
            It is an important method that makes key parallels between
            machine learning and game theory, and provides solid theoretical 
            foundation for feature attribution methods. Expected gradients
            is a direct extension of integrated gradients, and wouldn't exist
            without it.
        </p>
        <p>
            Instead, our hope is that this article provides more
            insight about what the choice of baseline \(x'\) means,
            and to convince you that the choice is important
            when you are interpreting your machine learning models. If you
            have a natural way to define \(x'\) in your particular application,
            that's great, and you shouldn't hesitate to use it. But if you don't,
            consider letting your data decide for you.
            <d-figure>
                <figure class='l-body'>
                    <div id='figure7_div'></div>
                    <script src="figure7.js"></script>
                    <figcaption>
                        A side-by-side comparison of Integrated Gradients
                        and Expected Gradients.
                    </figcaption>
                </figure>
            </d-figure>
            
        </p>
        </d-article>
        <d-appendix>
            <h3>Acknowledgments</h3>
            <p>
              I'm sure I have people I need to acknowledge here,
              but I can't remember who right now. I'll get to this later.
            </p>
          
            <d-footnote-list></d-footnote-list>
            <d-citation-list></d-citation-list>
        </d-appendix>

          <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
        <d-bibliography src="bibliography.bib"></d-bibliography>

        <script type="text/bibliography">
        
        </script>
    </body>
</html>