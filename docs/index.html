<!DOCTYPE HTML>
<html xml:lang="en" lang="en">
<head>
  <title>CSE512 | A3 [Add: title]</title>
  <style media="all">
* { padding: 0; margin: 0; }
 body {
  margin: 0 auto 0 auto;
  padding: 0;
  max-width: 1200px;
  font-family: "Avenir", "Avenir Next", Helvetica Neue, Arial;
  font-size: 0.95em;
}
 a, a:visited { text-decoration: none; color: #7533f4; }
a:hover { text-decoration: underline; color: #f4b014; }
 h1, h2, h3, h4, h5 {
  color: #492a7c;
  background-color: inherit;
  font-weight: normal;
  padding: 0 0 5px 0;
  margin: 15px 0 0 0;
  border: none;
  clear: right;
}
h1 { font-size: 24pt; margin:  5px 0 10px 0; line-height: 28px; }
h2 { font-size: 14pt; margin: 30px 0 15px 0; letter-spacing: 0.01em; border-bottom: 1px solid #ccc;  line-height: 20px;}
h3 { font-size: 13pt; }
h4 { font-size: 12pt; }
h5 { font-size: 11pt; }
p { margin: 0 0 10px 0; }
 .content {
  margin: 0;
  padding: 15px 20px;
  background-color: #ffffff;
}
 .title, .title h1, .title a {
  color: #492a7c;
  font-size: 24pt;
  margin-bottom: 20px;
  margin-top: 5px;
}
 .footer {
  border-top: 1px solid #ccc;
  margin-top: 30px;
  padding-top: 4px;
  text-align: right;
  font-size: 12px;
}
.footer a {
  color: #21346B;
}
.footer a:hover {
  color: #ce3333;
}
  </style>
</head>
<body>
<div class="content">
   <section class="title">
    <a href="/">Visualizing Gradient Attributions</a>
  </section>

   <section>
    <p>
      <strong>Team Members</strong>: Pascal Sturmfels
    </p>
  </section>

   <section>
    <em>[Add: a summary figure of your project. Be sure to include relevant captions, too.]</em>
    <p><em>[Add: links to your paper and poster.]</em></p>
  </section>

  <section>
    <h2>Abstract</h2>
    <em>[Add: a short description of your project, ideally within 150 words.]</em>
  </section>

  <!-- <section>
    <h2>Running Instructions</h2>
    <em>[Add: running instructions for the software and other optional materials.]</em>
  </section> -->
  <section>
    <h2>READ ME</h2>
    <p>
        This project is meant to be read like a blog-post, or as a workshop-style submission.
        I describe some of the concepts I think about for research, and I plan to add a portion
        that describes a novel approach our lab has proposed. Throughout the text, I link 
        to visualizations (<a href='figure1.html' target='_blank'>Figure 1</a>, 
        <a href='figure2.html' target='_blank'>Figure 2</a>,
        <a href='figure3.html' target='_blank'>Figure 3</a>,
        and 
        <a href='figure4.html' target='_blank'>Figure 4</a>) that illustrate the
        how an existing method for interpreting deep neural networks works, and 
        some of the issues with said method. I've added context for each figure in the paragraphs below.
        I plan to add two more figures that describe the novel method we've proposed.
        <br><br>
        
        I haven't yet had time to merge the figures into the text. I plan to do so for the
        final submission, but for now, you can view their separate HTML pages 
        by clicking the links in the text below.
    </p>
  </section>
  
  <section>
      <h2> Visualizing Gradient-Based Atttribution Methods in Image Classification Networks</h2>
      
      <h4>Neural networks are good at image classification</h4>
      <p>
          In recent years, machine learning methods have gotten increasingly good at
          a huge variety of tasks, including image classification: determining what object
          is an image. This is in part due to the resurgence of neural networks. Specifically,
          convolutional neural networks (CNNs) are the most popular class of model used
          for image-related tasks. Inception V4, an image classification network trained to do
          image classification on the ImageNet dataset can achieve a validation accuracy of over
          80%. This means that, on images it has never seen before, the network can correctly
          predict which object is in the image given a list of 1000 possible classes, over 80% of the time.
          In <a href='figure1.html' target='_blank'>Figure 1</a>, I plot the output of the Inception V4
          network on 4 different validation images. The bar plot shows the top 5 objects that the network
          thinks the image represents, in sorted order. The network gets all of the images correct!
      </p>
      
      <h4>But how did the network make the prediction it did?</h4>
      <p>
          Although it is great that machine learning practitioners can develop accurate networks,
          users often want to know <em>why</em> a network made a prediction it did. This is especially
          useful in high-risk situations: if a model told diagnosed you with a disease, you'd 
          want the model to be able to explain its decision making process! Even in image classification,
          explanations are important: did the network classify a bird correctly because it learned
          what birds looks like, or because pictures of birds typically have green backgrounds?
          Feature attribution methods provide us with a way to answer these questions. Given a prediction,
          feature attribution methods determine which features were most important to making the prediction.
          In image classification, the features are individual pixels in the image. In 
          <a href='figure2.html' target='_blank'>Figure 2</a>, we plot the validation images 
          side-by-side with a saliency map: a map of which pixels were most important to the network
          when making the prediction. White pixels are more important, and black pixls are less important.
          The figure demonstrates that for some images, the network behaves intuitively and highlights the
          object in the image, but for others, it seems to use arbitrary pixels in the background when making
          the prediction.
      </p>
      
      <h4>How did we generate feature attribution maps?</h4>
      <p>
          How were we able to plot a map of which pixels the Inception V4 model thought was important
          when making predictions? It turns out that there exists several methods for
          generating feature attributions for deep neural networks. One of the most 
          popular is called Integrated Gradients, and the way it works is by using the gradient.
          Recall that the gradient of a multi-variate function is the direction of maximum increase.
          This means that the gradients tell us which pixels that, if we changed them,
          would most change the output of the network! The way integrated gradients works
          is that it defines a baseline image x'. Then, it integrates all of the gradients
          between x' and the current image x. For a variety of mathematical reasons, this turns out
          to well-approximate which pixels the network is using when making a prediction on the image x.
          
          <br><br>
          To better understand the process, we instead opt to 
          visualize it in <a href='figure3.html' target='_blank'>Figure 3</a>. The first image
          shows the image x interpolated with the baseline image x', which we've arbitrarily chosen
          to be the constant black image. The second image shows the gradients at the interpolated image,
          which you can think of as what the network is <em>seeing</em> at that image. The third image 
          shows how the gradients accumulate from 0 to the current alpha value. This process is visualizing
          how the maps in the previous figure were generated: at alpha equal to 1, the accumulated
          gradients approximate which pixels are most important to the network's prediction. Finally,
          the line plot on the far right plots the sum of the cumulative gradients up to the current alpha.
          That is, it simply takes the third image, and sums all of the values in it. 
          Why would we care about this sum? It turns out that this sum indicates how good the approximation
          is: if the sum is close to the output of the network at that prediction (indicated in blue),
          then the acuumulated gradients up to alpha are a good approximation of what the network is doing.
          Plotting the sum in this way also demonstrates another fact: the majority of the magnitude of the gradients
          comes from very small alpha values: at levels that humans cannot distinguish the original image. 
          It is precisely at these levels that the network is seeing the most information!
          
          <br><br>
          
          For the sake of time, I've skipped over many of the mathematical details of 
          integrated gradients. If you would like a more formal definition of integrated gradients, see
          <a href='https://arxiv.org/abs/1703.01365' target='_blank'>the arXiv paper</a>.
      </p>
  </section>
  
  <section>
      <h4>What's this about a baseline image?</h4>
      <p>
          One crucial detail I skipped over is the baseline image x'. You can think about
          integrated gradients in the following way. Mathematically, it asks our network: "how did
          you get from the prediction you made on the baseline image x' to the prediction
          you made on the current image x?". We can re-phrase this as: how did you get from seeing
          <em>nothing</em> (all black image) to <em>something</em> (the image of a bird, for example).
          However, you might realize a problem with this approach. Why does the color black 
          indicate nothing? In images, the color black can indicate the presence of many things.
          It is an arbitrary color, no better than using red or green to indicate nothingness!
          This is the pitfall of integrated gradients. If you want to understand your network,
          you have to define some notion of "nothingness". It is not clear how to do so with images.
          In fact, if we use different colors in our reference image, we can get 
          entirely different explanations for our network! Because of how integrated gradients works,
          it is biased against the color that is chosen as a reference. If I choose black as a reference value,
          than black pixels in my image won't be highlighted as important even if they should be.
          We demonstrate this in <a href='figure4.html' target='_blank'>Figure 4</a>. Mouse over the 
          segmented image to choose a reference color to use for integrated gradients, and observe
          that the pixels that are highlighted as important can change drastically depending on which
          color is used as a background reference.
      </p>
  </section>
  
  <section>
      <h4>A new approach: expected gradients</h4>
      <p>
          Expected gradients is a new approach to overcome the problems I've described above.
          In this section, I will visualize how expected gradients works, and why 
          it overcomes the aforementioned problems. These visualizations are still in progress,
          but will look similar to the ones above.
      </p>
  </section>

  <div class="footer">
    <a href="https://courses.cs.washington.edu/courses/cse512/19sp/">CSE 512 Data Visualization</a>
    <a href="http://www.washington.edu">University of Washington</a>
  </div>
 </div>
</body>
</html> 

