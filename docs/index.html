<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <script src="https://distill.pub/template.v1.js"></script>
        <script src="https://d3js.org/d3.v5.min.js"></script>
        <script src="https://d3js.org/d3-collection.v1.min.js"></script>
        <script src="https://rawgit.com/nstrayer/slid3r/master/dist/slid3r.js"></script>
        <script src="cross_fade.js"></script>
        
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>

        <script type="text/front-matter">
          title: "Data-Driven Feature Attributions with Expected Gradients"
          description: "Feature Attrtibution Methods are a popular way of explaining
             deep models. Most require choosing a
             hyper-parameter known as the 'baseline input'.
             What does this hyper-parameter mean, and how important is it? In this article,
             we investigate these questions, and discuss why it is useful 
             to extend the notion of a baseline from a single input to a distribution."
          authors:
          - Pascal Sturmfels: http://psturmfels.github.io
          - Scott Lundberg: http://scottlundberg.com
          - Su-In Lee: https://suinlee.cs.washington.edu
          affiliations:
          - University of Washington: https://www.cs.washington.edu
          - University of Washington: https://www.cs.washington.edu
          - University of Washington: https://www.cs.washington.edu
        </script>
    </head>
    <dt-article>
        <h1>Data-Driven Feature Attributions with Expected Gradients</h1>
        <p>Feature Attrtibution Methods are a popular way of explaining
           deep models. Most require choosing a
           hyper-parameter known as the "baseline input".
           What does this hyper-parameter mean, and how important is it? In this article,
           we investigate these questions, and discuss why it is useful 
           to extend the notion of a baseline from a single input to a distribution. </p>
        <dt-byline></dt-byline>
        <h2>Introduction</h2>
        <p>
            If you are in the business of training neural networks,
            you might have heard of the integrated gradients method, which
            was introduced at ICML two years ago 
            <dt-cite key="sundararajan2017axiomatic"></dt-cite>.
            The method computes which features are important 
            to a neural network in making a prediction on a 
            particular data point, helping users
            understand what patterns their models are learning.
            Since its introduction,
            integrated gradients has been used to interpret 
            networks trained on a variety of data types, 
            including retinal fundus images <dt-cite key="sayres2019using"></dt-cite>
            and electrocardiogram recordings <dt-cite key="warrick2018ensembling"></dt-cite>.
        </p>
        <p>
            If you've ever used integrated gradients, 
            or for that matter, any of a similar family of
            attribution methods <dt-cite key="lundberg2017unified,binder2016layer,shrikumar2017learning"></dt-cite>,
            you also know that you need to define a baseline input \(x'\) before
            using the method. There is scarce information about this hyper-parameter choice
            in aforementioned literature. Is integrated gradients sensitive to the 
            hyper-parameter choice? Are there any guidelines for choosing \(x'\) other
            than setting it to be a vector of zeros?
            And why does the method require this hyper-parameter in the first place?
        </p>
        <p>
            In this article, we will delve into how this hyper-parameter choice arises,
            and why understanding it is important when you are doing model interpretation.
            As a case-study, we will focus on image classification models in order 
            to visualize the effects of the baseline input. Our visualizations will
            lead us to describe an extension of integrated gradients, called <i>expected gradients</i>, which we 
            introduced in <dt-cite key="erion2019learning"></dt-cite>. In brief,
            expected gradients extends integrated gradients by using a data distribution
            as a baseline instead of a single input.
        </p>
        <h2>Image Classification</h2>
        <p>
          We focus on image classification as a task, as it will allow us to visually
          plot integrated gradients attributions, and compare them with our intuition
          about which pixels we think should be important. We use on the Inception V4 architecture 
          <dt-cite key="szegedy2017inception"></dt-cite>, a convolutional 
          neural network designed for the ImageNet dataset <dt-cite key="deng2009imagenet"></dt-cite>,
          in which the task is to determine which class an image belongs to out of 1000 classes.
          On the ImageNet validation set, Inception V4 has a top-1 accuracy of over 80%.
          We download weights from TensorFlow-Slim <dt-cite key="silberman2016tensorflow"></dt-cite>,
          and visualize the predictions of the network on four different images from the 
          validation set.
          <figure>
              <div id='figure1_div'></div>
              <script src="figure1.js"></script>
              <figcaption>
                  The Inception V4 network classifies all images above correctly with high confidence.
              </figcaption>
          </figure>
        </p>
        <p>
          Although our models perform very well on unseen data,
          users may still be left wondering: <i>how</i> did the model make
          the prediction it did? This question may be critical when models
          are making high stakes decisions about, say, an individual's health or finances.
          Although there are many ways to interpret deep neural networks, here we focus 
          on feature attributions.
        </p>
        <p>
          Given a target input \(x\) and a network function \(f\), 
          feature attribution methods assign a weight \(\phi_i^{IG}(f, x)\)
          to the \(i\)-th feature value representing how important that
          feature was toward computing \(f(x)\). There are many such methods for
          neural networks, including DeepLIFT <dt-cite key="shrikumar2017learning"></dt-cite>,
          Layer-wise Relevance Propagation <dt-cite key="binder2016layer"></dt-cite>
          and Integrated Gradients <dt-cite key="sundararajan2017axiomatic"></dt-cite>.
        </p>
        <p>
            Below, we visualize which pixels were most important to the network's correct
            prediction using Integrated Gradients.<dt-fn>Throughout this paper, we 
            use 50 samples when approximating any integral or expectation.</dt-fn>
            The pixels in white indicate more important pixels. Note here
            that the feature attributions are capped at the 99th percentile to avoid
            high-magnitude attributions dominating the color scheme, as done in 
            <dt-cite key="smilkov2017smoothgrad"></dt-cite>.
            <figure class="l-body">
                <div id='figure2_div'></div>
                <script src="figure2.js"></script>
                <figcaption>
                    Pixel-wise attributions of the Inception V4 network using Integrated Gradients.
                    You might notice that some attributions highlight pixels that do not seem important
                    relative to the true class label.
                </figcaption>
            </figure>
        </p>
        <h2>A Better Understanding of Integrated Gradients</h2>  
        <p>
          As you look through the attribution maps, you might find some of them
          unintuitive. To better understand this behavior, we need to explore how
          we generated feature attributions. Formally, Integrated Gradients
          defines the importance value for the \(i\)th feature value as follows:
          $$\phi_i^{IG}(f, x, x') = (x_i - x'_i) \times \int_{\alpha = 0}^ 1
            \frac{\delta f(x' + \alpha (x - x'))}{\delta x_i} d \alpha
          $$
          where \(x\) is the current input,
          \(f\) is the model function and \(x'\) is some baseline input that is meant to represent 
          "absence" of feature input. The subscript \(i\) is used
          to denote indexing into the \(i\)th feature.
        </p>
        <p>
            To better understand how this summation accumulates
            feature importances between the baseline input and the current input,
            we can break it down and visualize it in three separate parts:
            $$
                \tag{1} x' + \alpha (x - x')
            $$
            $$
                \tag{2} (x_i - x'_i) \times \frac{\delta f(x' + \alpha (x - x'))}{\delta x_i}
            $$
            $$
                \tag{3} (x_i - x'_i) \times \int_{\alpha' = 0}^{\alpha} \frac{\delta f(x' + \alpha' (x - x'))}{\delta x_i} d \alpha'
            $$
            We visualize these three equations below.<dt-fn>Note that in practice, we use a discrete sum
            approximation of the integral with 50 linearly-spaced points between 0 and 1.</dt-fn>
            <figure class="l-page-outset">
                <div id='figure3_div'></div>
                <script src="figure3.js"></script>
                <figcaption>
                    Integrated Gradients, visualized. In the line chart, the red line refers to
                    equation (4) and the blue line refers to \(f(x) - f(x')\). Notice how high magnitude gradients
                    accumulate at small values of \(\alpha\).
                </figcaption>
            </figure>
        </p>
        <p>
            The line chart above plots the following equation:
            $$
                \tag{4} \sum_i \bigg( (x_i - x'_i) \times \int_{\alpha' = 0}^{\alpha} 
                \frac{\delta f(x' + \alpha' (x - x'))}{\delta x_i} d \alpha' \bigg)
            $$
            That is, it sums all of the pixel attributions in equation (3).
            It turns out that Integrated Gradients attributions have
            the following property. If we were able to compute the true integral,
            we would find that:
            $$
                \sum_i \phi_i^{IG}(f, x, x') = f(x) - f(x')
            $$ 
            This means that we can plot the summation of attributions over \(\alpha\)
            and see both how good our approximation of the integral is. But this property
            goes deeper than that: it is one of several axioms that uniquely characterize 
            Integrated Gradients (and, under different assumptions,
             <a href="https://github.com/slundberg/shap" target="_blank"></a>SHAP 
            <dt-cite key="lundberg2017unified"></dt-cite>). 
        </p>
        
        <h3>The Aumann-Shapley Value</h3>
        <p>
            Integrated Gradients is inspired by work
            from cooperative game theory, specifically the Aumann-Shapley value
            <dt-cite key="aumann2015values"></dt-cite>. In cooperative game theory,
            a non-atomic game is a construction used to model large-scale economic systems
            where there are enough participants that it is desirable to model them continuously.
        </p>
        <p>
            Formally, let \(I\) be an infinite set of players,
            and \(C\) be the \(\sigma\)-algebra on \(I\). We define
            a game \(v\) as a real-valued function on \(C\) such that
            \(v(\varnothing) = 0\). The Aumann-Shapley value
            defines a way to allocate credit among the players \(i \in I\).
            We can use these values to, for example, determine which
            players are most responsible for the output of an economic system.
        </p>
        <p>
            This foray into cooperative game theory might seem both abstract and vague.
            It isn't important to understand all of the game theoretic background. The important
            take away is this: <b>games are functions defined on sets of players</b>. 
            Given a set \(S \in C\) and a player \(i \in S\), we can evaluate
            \(v\) both with \(i\), e.g. \(v(S)\), and more critically, without
            \(i\), e.g. \(v(S \setminus i)\).
        </p>
        <p>
            This fact is important because, when we translate these methods into
            interpreting machine learning models, we draw an analogy between
            <i>games</i> and <i>models</i>. A model is similar to a game, the
            input features to a model are similar to players in a game, and the
            output of a model is analogous to the value of a game. The Aumann-Shapley
            Value is used to allocate credit to players among a game, and analogously, 
            Integrated Gradients allocates credit among input features to a model.
        </p>
        <p>
            The only tricky part
            is this: how do you evaluate a model \(f\) on a <i>subset</i> of features?
            It is easy to do this with games: they are by definition functions
            over subsets of players. But how would you evaluate a deep network
            on subsets of its input features? Most deep learning models
            cannot handle arbitrary patterns of missingness.
        </p>
        
        <p>
            "Aha!" you say. "To simulate missingness, simply replace the value of missing features
            with 0!" It turns out, this is exactly what most people do when they 
            use Integrated Gradients. It appears to be common practice
            to choose a baseline input \(x'\) to be the vector of
            all zeros. But consider the following scenario: you've learned a model
            on a healthcare dataset, and one of the features is blood sugar level.
            The model has correctly learned that excessively low levels of blood sugar,
            which correspond to hypoglycemia, is dangerous. Do you think
            a blood sugar level of 0 is the appropriate value to model missingness?
        </p>
        <p>
            The point here is that fixed feature values may have unintended meaning.
            The problem compounds further when you consider the mathematical formulation
            of integrated gradients: it has a multiplicative \(x_i - x'_i\) term.
            For the sake of a thought experiment, suppose a patient had a blood sugar level of 0. 
            To understand why our machine learning model thinks this patient
            is at high risk, you run Integrated Gradients on this data point with a
            baseline of the all-zeros vector. The blood sugar level of the patient would have 0 feature importance,
            because \(x_i - x'_i = 0\). This is despite the fact that 
            a blood sugar level of 0 would be fatal!
        </p>
        <p>
          We find similar problems when we move to the image domain.
          The all-zeros image is a constant black image, which means
          that Integrated Gradients with this baseline 
          tends not to highlight black or dark pixels as important.
          In fact, depending on the baseline input you choose,
          which pixels are highlighted as important can vary drastically, as
          illustrated below.
          <figure class="l-page-outset">
              <div id='figure4_div'></div>
              <script src="figure4.js"></script>
              <figcaption>
                  Mouse over the segmented image to choose a different color
                  as a baseline input \(x'\). Notice that pixels
                  of the baseline color are not highlighted as important, 
                  even if they make up part of the main object in the image.
              </figcaption>
          </figure>
        </p>
        <h2>Integrating out the Baseline</h2>
        <p>
            It seems like no matter what we choose \(x'\) to be,
            we will run into this issue: there will be a color
            that Integrated Gradients does not highlight as important. 
            This issue is not specific to Integrated Gradients - 
            the previously mentioned DeepLIFT and Layer-wise Relevance Propagation also
            require a baseline input. This baseline input allows us to compute the 
            importance of each feature's presence to the output.
        </p>
        <p>
            So is it possible to define a more meaningful notion of a missing feature?
            Lets suppose you were trying to predict how many inches of rain would fall
            tomorrow. Like most weather apps, you might aggregate some data, make a statistical
            model that gives a distribution over amount of rainfall, and then
            use the mean of that distribution as your prediction. But wouldn't it
            be so much more informative to simply describe the distribution your model predicted? 
            Either way, the amount of rainfall
            tomorrow is an unknown, "missing" quantity.
        </p>
        <p>
            In supervised learning, we make the assumption that the data is drawn
            from some distribution \(D\). This assumption that the training and testing data 
            share a common, underlying distribution is what allows us to 
            do supervised learning and make claims about generalizability. If
            the data distribution has density \(p_D\), we don't need to
            model missingness using a constant value. We can use \(p_D\)
            to integrate out the choice of baseline:
            $$ \phi_i(f, x, x') = \int_{x'} \bigg( (x_i - x'_i) \times \int_{\alpha = 0}^ 1
              \frac{\delta f(x' + \alpha (x - x'))}{\delta x_i} d \alpha \bigg) p_D(x') dx'
            $$
        </p>
        <p>
            In practice, we do not have access to the underlying distribution \(p_D\).
            However, we can observe that both of these integrals integrate over distributions.
            The first integrates over \(p_D\), and the second integrates over the uniform distribution.
            By thinking about these integrals as expectations, we arrive 
            at our proposed extension of Integrated Gradients, called <b>Expected 
            Gradients</b>, which we introduced in <dt-cite key="erion2019learning"></dt-cite>.<dt-fn>
            Our work that introduces Expected Gradients does not actually discuss the nuances presented here. Instead, 
            it is about how you can train models to be 
            more interpretable and achieve better generalization performance by imposing priors
            on their feature attributions.
            </dt-fn> We collapse both expectations to get the formal definition
            of Expected Gradients:
            $$ \phi_i^{EG}(f, x, x') = \mathop{\mathbb{E}}_{x' \sim D, \alpha \sim U(0, 1)} \bigg[ (x_i - x'_i) \times 
              \frac{\delta f(x' + \alpha (x - x'))}{\delta x_i} \bigg]
            $$
        </p>
        <p>
          Under the assumption that our training dataset represents 
          random samples from \(D\), we can draw \(x'\) randomly
          from our training set, \(\alpha\) from the uniform distribution and
          use these samples to approximately compute the expectation above.
          We illustrate this sampling process by breaking it down into three equations:
          $$
            \tag{1} x'^k + \alpha (x - x'^k)
          $$
          $$
            \tag{2} (x_i - x'^k_i) \times 
              \frac{\delta f(x'^k + \alpha (x - x'^k))}{\delta x_i}
          $$
          $$
            \tag{3} \frac{1}{k} \sum_{j=1}^k (x_i - x'^j_i) \times 
              \frac{\delta f(x'^j + \alpha (x - x'^j))}{\delta x_i}
          $$
          where \(x'^k\) and \(x'^j\) represent the \(k\)th and \(j\)th
          samples from \(D\), and \(k\) is the current sample number.
          <figure class="l-page-outset">
              <div id='figure6_div'></div>
              <script src="figure6.js"></script>
              <figcaption>
                  A visual representation of Expected Gradients. Instead of taking contributions
                  from a single path, Expected Gradients averages contributions from 
                  all paths defined by the underlying data distribution.
              </figcaption>
          </figure>
        </p>
        
        <p>
          Integrated Gradients is part of a family of methods called path methods,
          which integrate contributions along a path between two points. It turns out
          that, if you are generating feature attributions
          relative to some baseline, diagonal path methods are the only methods that have certain theoretical
          guarantees <dt-cite key="friedman2004paths"></dt-cite>. So does Expected Gradients come with these same theoretical guarantees?
          It turns out it does. The key difference is that Expected Gradients
          allows contributions over multiple paths, not the single path between a
          fixed \(x'\) and \(x\). Other than that, it is still also a diagonal path
          method, and in expectation, satisfies the same theoretical guarantees that 
          Integrated Gradients does.
        </p>
        <p>
            If you are interested in using Expected Gradients to explain
            your machine learning models, head over 
            to the <a href="http://github.com/slundberg/shap" target="_blank">SHAP package</a> 
            <dt-cite key="lundberg2017unified"></dt-cite>,
            where it is implemented as the <dt-code language="python">GradientExplainer</dt-code>
            object for TensorFlow, Keras, and PyTorch models.
        </dt>
        </p>
        
        <h2>Conclusion</h2>
        <p>
            The purpose of this article is not to criticize Integrated Gradients.
            Its an important method that makes key parallels between
            machine learning and game theory, and provides solid theoretical 
            foundation for feature attribution methods. Expected Gradients
            is a direct extension of Integrated Gradients, and wouldn't exist
            without it.
        </p>
        <p>
            Instead, our hope is that this article provides more
            insight about what the choice of baseline \(x'\) means,
            and to convince you that the choice is important
            when you are interpreting your machine learning models. If you
            have a natural way to define \(x'\) in your particular application,
            that's great, and you shouldn't hesitate to use it. But if you don't,
            consider letting your data decide for you.
            <figure>
                <div id='figure7_div'></div>
                <script src="figure7.js"></script>
                <figcaption>
                    A side-by-side comparison of Integrated Gradients
                    and Expected Gradients.
                </figcaption>
            </figure>
        </p>
        </dt-article>
        
        <dt-appendix>
        </dt-appendix>

        <script type="text/bibliography">
        @inproceedings{sundararajan2017axiomatic,
            title={Axiomatic attribution for deep networks},
            author={Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
            booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
            pages={3319--3328},
            year={2017},
            organization={JMLR. org},
            url={https://arxiv.org/pdf/1703.01365}
        }
        
        @inproceedings{szegedy2017inception,
          title={Inception-v4, inception-resnet and the impact of residual connections on learning},
          author={Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A},
          booktitle={Thirty-First AAAI Conference on Artificial Intelligence},
          year={2017},
          url={https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/viewFile/14806/14311}
        }
         
        @inproceedings{deng2009imagenet,
            title={Imagenet: A large-scale hierarchical image database},
            author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
            booktitle={2009 IEEE conference on computer vision and pattern recognition},
            pages={248--255},
            year={2009},
            organization={Ieee},
            url={https://www.researchgate.net/profile/Li_Jia_Li/publication/221361415_ImageNet_a_Large-Scale_Hierarchical_Image_Database/links/00b495388120dbc339000000/ImageNet-a-Large-Scale-Hierarchical-Image-Database.pdf}
        }

        @inproceedings{he2015delving,
          title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
          author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
          booktitle={Proceedings of the IEEE international conference on computer vision},
          pages={1026--1034},
          year={2015},
          url={https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf}
        }
        
        @article{silberman2016tensorflow,
          title={Tensorflow-slim image classification model library},
          author={Silberman, Nathan and Guadarrama, Sergio},
          year={2016},
          publisher={Accessed},
          url={https://github.com/tensorflow/models/tree/master/research/slim}
        }
        
        @inproceedings{shrikumar2017learning,
          title={Learning important features through propagating activation differences},
          author={Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
          booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
          pages={3145--3153},
          year={2017},
          organization={JMLR. org},
          url={https://arxiv.org/pdf/1704.02685}
        }
        
        @inproceedings{binder2016layer,
          title={Layer-wise relevance propagation for neural networks with local renormalization layers},
          author={Binder, Alexander and Montavon, Gregoire and Lapuschkin, Sebastian and Muller, Klaus-Robert and Samek, Wojciech},
          booktitle={International Conference on Artificial Neural Networks},
          pages={63--71},
          year={2016},
          organization={Springer},
          url={https://arxiv.org/pdf/1604.00825.pdf}
        }
        
        @article{smilkov2017smoothgrad,
          title={Smoothgrad: removing noise by adding noise},
          author={Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Viegas, Fernanda and Wattenberg, Martin},
          journal={arXiv preprint arXiv:1706.03825},
          year={2017},
          url={https://arxiv.org/pdf/1706.03825.pdf}
        }
        
        @inproceedings{lundberg2017unified,
          title={A unified approach to interpreting model predictions},
          author={Lundberg, Scott M and Lee, Su-In},
          booktitle={Advances in Neural Information Processing Systems},
          pages={4765--4774},
          year={2017},
          url={http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf}
        }
        
        @book{aumann2015values,
          title={Values of non-atomic games},
          author={Aumann, Robert J and Shapley, Lloyd S},
          year={2015},
          publisher={Princeton University Press}
        }
        
        @article{friedman2004paths,
          title={Paths and consistency in additive cost sharing},
          author={Friedman, Eric J},
          journal={International Journal of Game Theory},
          volume={32},
          number={4},
          pages={501--518},
          year={2004},
          publisher={Springer},
          url={https://link.springer.com/content/pdf/10.1007/s001820400173.pdf}
        }
        
        @misc{erion2019learning,
            title={Learning Explainable Models Using Attribution Priors},
            author={Gabriel Erion and Joseph D. Janizek and Pascal Sturmfels and Scott Lundberg and Su-In Lee},
            year={2019},
            eprint={1906.10670},
            archivePrefix={arXiv},
            primaryClass={cs.LG},
            url={https://arxiv.org/pdf/1906.10670.pdf}
        }
        
        @article{olah2018the,
          author = {Olah, Chris and Satyanarayan, Arvind and Johnson, Ian and Carter, Shan and Schubert, Ludwig and Ye, Katherine and Mordvintsev, Alexander},
          title = {The Building Blocks of Interpretability},
          journal = {Distill},
          year = {2018},
          note = {https://distill.pub/2018/building-blocks},
          doi = {10.23915/distill.00010}
        }
        
        @article{olah2017feature,
          author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
          title = {Feature Visualization},
          journal = {Distill},
          year = {2017},
          note = {https://distill.pub/2017/feature-visualization},
          doi = {10.23915/distill.00007}
        }
        
        @article{kim2017interpretability,
          title={Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)},
          author={Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and Sayres, Rory},
          journal={arXiv preprint arXiv:1711.11279},
          year={2017}
        }
        
        @inproceedings{fong2017interpretable,
          title={Interpretable explanations of black boxes by meaningful perturbation},
          author={Fong, Ruth C and Vedaldi, Andrea},
          booktitle={Proceedings of the IEEE International Conference on Computer Vision},
          pages={3429--3437},
          year={2017}
        }
        
        @inproceedings{selvaraju2017grad,
          title={Grad-cam: Visual explanations from deep networks via gradient-based localization},
          author={Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
          booktitle={Proceedings of the IEEE International Conference on Computer Vision},
          pages={618--626},
          year={2017}
        }
        
        @inproceedings{zeiler2014visualizing,
          title={Visualizing and understanding convolutional networks},
          author={Zeiler, Matthew D and Fergus, Rob},
          booktitle={European conference on computer vision},
          pages={818--833},
          year={2014},
          organization={Springer}
        }
        
        @article{sayres2019using,
          title={Using a deep learning algorithm and integrated gradients explanation to assist grading for diabetic retinopathy},
          author={Sayres, Rory and Taly, Ankur and Rahimy, Ehsan and Blumer, Katy and Coz, David and Hammel, Naama and Krause, Jonathan and Narayanaswamy, Arunachalam and Rastegar, Zahra and Wu, Derek and others},
          journal={Ophthalmology},
          volume={126},
          number={4},
          pages={552--564},
          year={2019},
          publisher={Elsevier},
          url={https://www.sciencedirect.com/science/article/pii/S0161642018315756}
        }
        @article{warrick2018ensembling,
          title={Ensembling convolutional and long short-term memory networks for electrocardiogram arrhythmia detection},
          author={Warrick, Philip A and Homsi, Masun Nabhan},
          journal={Physiological measurement},
          volume={39},
          number={11},
          pages={114002},
          year={2018},
          publisher={IOP Publishing},
          url={https://iopscience.iop.org/article/10.1088/1361-6579/aad386/meta}
        }
        </script>
    </body>
</html>